{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reporte_tutorial_3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXOG_lL-_rKc"
      },
      "source": [
        "## Práctica 02, Reporte 01\n",
        "## Daniel Ramírez Umaña\n",
        "## 13 Septiembre 2021"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0idwj79_xLK"
      },
      "source": [
        "## How to Remove Outliers for Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bhIchGE_2Aw"
      },
      "source": [
        "Limpiamos nuestros conjuntos con la finalidad de que estos sean lo más representativos posible del problema.\n",
        "\n",
        "En muchos casos los conjuntos tienen valores que se alejan mucho de los demás datos y que se comportan de forma extraña. A estos los llamamos **valores atípicos** o bien ***outliers***."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRz1TyyYBVGQ"
      },
      "source": [
        "### 1. Qué es un Valor Atípico (outlier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7BxrDMDCX38"
      },
      "source": [
        "Este es una observación que de una forma u otra no se parece a las demás o bien que no encaja de alguna manera.\n",
        "\n",
        "Algunas de las causas de un valor atípico son:\n",
        "* Error de medición o de entrada.\n",
        "* Corrupción de datos.\n",
        "* Observación atípica real.\n",
        "\n",
        "Si bien no hay forma precisa para detectar *outliers*, es posible utilizar métodos estadísticos para identificar observaciones que parecieran serlas en relación a los datos disponibles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO5ZXgv4BcJK"
      },
      "source": [
        "### 2. Conjunto de Datos de Prueba"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhNOQ1DYHVAU",
        "outputId": "391383bd-fcc7-4517-d71c-f1d0801741b3"
      },
      "source": [
        "# generate gaussian data\n",
        "from numpy.random import seed\n",
        "from numpy.random import randn\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "# seed the random number generator\n",
        "seed(1)\n",
        "# generate univariate observations\n",
        "data = 5 * randn(10000) + 50\n",
        "# summarize\n",
        "print('mean=%.3f stdv=%.3f' % (mean(data), std(data)))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean=50.049 stdv=4.994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDR06LesBnQu"
      },
      "source": [
        "### 3. Método de la Desviación Estándar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "901JL47nHta9"
      },
      "source": [
        "Es posible usar la desviación estándar de la muestra como límite para identificar los valores atípicos, si sabemos que la distribución de los valores de la muestra es Gaussiana o similar a la Gaussiana."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFGVL1_NI4JQ"
      },
      "source": [
        "Podemos calcular la media y la desviación estándar de una muestra dada, y luego calcular el punto de corte para identificar los valores atípicos como más de 3 desviaciones estándar de la media."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSY8yTg2HssC",
        "outputId": "953bf4b3-001a-4f29-f175-0c8ccacc0186"
      },
      "source": [
        "# identify outliers with standard deviation\n",
        "from numpy.random import seed\n",
        "from numpy.random import randn\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "# seed the random number generator\n",
        "seed(1)\n",
        "# generate univariate observations\n",
        "data = 5 * randn(10000) + 50\n",
        "# calculate summary statistics\n",
        "data_mean, data_std = mean(data), std(data)\n",
        "# identify outliers\n",
        "cut_off = data_std * 3\n",
        "lower, upper = data_mean - cut_off, data_mean + cut_off\n",
        "\n",
        "# identify outliers\n",
        "outliers = [x for x in data if x < lower or x > upper]\n",
        "print('Identified outliers: %d' % len(outliers))\n",
        "\n",
        "# remove outliers\n",
        "outliers_removed = [x for x in data if x >= lower and x <= upper]\n",
        "print('Non-outlier observations: %d' % len(outliers_removed))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified outliers: 29\n",
            "Non-outlier observations: 9971\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2g7jRgpJwcl"
      },
      "source": [
        "Como podemos observar, 29 valores correspondientes a muy posibles *outliers* han sido eliminados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrnTRqd-Bzt6"
      },
      "source": [
        "### 4. Método del Rango Intercuartil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OZWcO6GKEOq"
      },
      "source": [
        "Aveces no es recomendable extraer valores tratándolos con una distribución gaussiana, como lo es el caso en que los datos no son normales o no lo suficientemente normales. Para estos casos podemos usar el **Rango Intercuartil** (IQR) el cual corresponde a la diferencia entre los percentiles 25 y 75 de los datos y se dedine la caja en un gráfico de cajas y bigote (box in a box and whisker plot)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xQ77BysLdvb",
        "outputId": "59120e87-9120-4b27-b5e0-7d129ccf1bc3"
      },
      "source": [
        "# identify outliers with interquartile range\n",
        "from numpy.random import seed\n",
        "from numpy.random import randn\n",
        "from numpy import percentile\n",
        "# seed the random number generator\n",
        "seed(1)\n",
        "# generate univariate observations\n",
        "data = 5 * randn(10000) + 50\n",
        "# calculate interquartile range\n",
        "q25, q75 = percentile(data, 25), percentile(data, 75)\n",
        "iqr = q75 - q25\n",
        "print('Percentiles: 25th=%.3f, 75th=%.3f, IQR=%.3f' % (q25, q75, iqr))\n",
        "# calculate the outlier cutoff\n",
        "cut_off = iqr * 1.5\n",
        "lower, upper = q25 - cut_off, q75 + cut_off\n",
        "# identify outliers\n",
        "outliers = [x for x in data if x < lower or x > upper]\n",
        "print('Identified outliers: %d' % len(outliers))\n",
        "# remove outliers\n",
        "outliers_removed = [x for x in data if x >= lower and x <= upper]\n",
        "print('Non-outlier observations: %d' % len(outliers_removed))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentiles: 25th=46.685, 75th=53.359, IQR=6.674\n",
            "Identified outliers: 81\n",
            "Non-outlier observations: 9919\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoD3VbYuLjE6"
      },
      "source": [
        "Podemos ver que para este conjunto y dado el IQR fueron eliminadas 81 observasiones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WfNF5xnB_6h"
      },
      "source": [
        "### 5. Detección Automática de Valores Atípicos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3MNqjACL0Ms"
      },
      "source": [
        "En **aprendizaje automático** o ***Machine Learning***, es posible realizar la clasificación de una clase para la detección de valores atípicos. A esto se le conoce como la **Clasificación de una clase** o ***One-Class Classification*** (OCC) que consiste en ajustar un modelo de datos \"normales\" y predecir de los nuevos datos son normales o un valor atípico.\n",
        "\n",
        "En casos de variables de baja dimensionalidad esta técnica podría funcionar, pero su fiabilidad se ve reducida con relación al aumento de características (a esto se le conoce como **la maldición de ls dimensionalidad**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZhTKWS2N_0f",
        "outputId": "0a09ca98-2621-496e-c6f9-e29d607e4582"
      },
      "source": [
        "# load and summarize the dataset\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "# load the dataset\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
        "df = read_csv(url, header=None)\n",
        "# retrieve the array\n",
        "data = df.values\n",
        "# split into inpiut and output elements\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "# summarize the shape of the dataset\n",
        "print(X.shape, y.shape)\n",
        "# split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
        "# summarize the shape of the train and test sets\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(506, 13) (506,)\n",
            "(339, 13) (167, 13) (339,) (167,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJ1V1bOgOLyK",
        "outputId": "31196f50-5162-4abe-f02d-0190d1deef87"
      },
      "source": [
        "# evaluate model on the raw dataset\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "# load the dataset\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
        "df = read_csv(url, header=None)\n",
        "# retrieve the array\n",
        "data = df.values\n",
        "# split into inpiut and output elements\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "# split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
        "# fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "# evaluate the model\n",
        "yhat = model.predict(X_test)\n",
        "# evaluate predictions\n",
        "mae = mean_absolute_error(y_test, yhat)\n",
        "print('MAE: %.3f' % mae)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: 3.417\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_lgBFEcOa5M",
        "outputId": "c85ee4c8-76e2-4196-f4ea-6ae6682caf59"
      },
      "source": [
        "# evaluate model on training dataset with outliers removed\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "# load the dataset\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
        "df = read_csv(url, header=None)\n",
        "# retrieve the array\n",
        "data = df.values\n",
        "# split into inpiut and output elements\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "# split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
        "# summarize the shape of the training dataset\n",
        "print(X_train.shape, y_train.shape)\n",
        "# identify outliers in the training dataset\n",
        "lof = LocalOutlierFactor()\n",
        "yhat = lof.fit_predict(X_train)\n",
        "# select all rows that are not outliers\n",
        "mask = yhat != -1\n",
        "X_train, y_train = X_train[mask, :], y_train[mask]\n",
        "# summarize the shape of the updated training dataset\n",
        "print(X_train.shape, y_train.shape)\n",
        "# fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "# evaluate the model\n",
        "yhat = model.predict(X_test)\n",
        "# evaluate predictions\n",
        "mae = mean_absolute_error(y_test, yhat)\n",
        "print('MAE: %.3f' % mae)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(339, 13) (339,)\n",
            "(305, 13) (305,)\n",
            "MAE: 3.356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDIVa5uUOeZm"
      },
      "source": [
        "Podemos ver el cambio en MAE una vez eliminados los *outliers*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqxBLjYZ_2oQ"
      },
      "source": [
        "### Lo aprendido y el posible uso en mi futuro profesional:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgyPMJ11OsjK"
      },
      "source": [
        "Aprendí que los valores atípicos pueden jugarme una mala pasada a la hora de entrenar mis modelos o bien a la hora de sacar conclusiones con respecto a mis conjuntos.\n",
        "\n",
        "En mi futuro profesional como Científico de Datos, será necesario tener en cuenta que debo chequear de la exxistancia de datos atípicos y tomar deseciones de qué hacer con ellos para mejorar mis modelos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDabx4ovAAIu"
      },
      "source": [
        "### Aspectos de los que me gustaría saber más o que no me quedaron muy claros:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6wJqgl-PPvL"
      },
      "source": [
        "Ya antes he trabajado con *outliers* en es sacar conclusiones con respecto a valores estadísticos, pero me gustaría ahora aplicarlo a un proyecto antes de utilizar un algoritmo clasificador."
      ]
    }
  ]
}