{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reporte_tutorial_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBiCh93lMaQH"
      },
      "source": [
        "## Práctica 03 &emsp;(23 de Septiembre)\n",
        "## &emsp;Parte 02\n",
        "## &emsp;Daniel Ricardo Ramírez Umaña, B45675"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y48QwHfhM6hM"
      },
      "source": [
        "## Parte 02 <br>Cómo Usar Extracción de Variables en Datos Tabulares para Aprendizaje Automático"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwQRSPQGMmBC"
      },
      "source": [
        "### &emsp;1. Lo aprendido:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aucwR0SVqq3"
      },
      "source": [
        "#### &emsp; 1. Técnicas de Extracción de Variables para Preparación de Datos\n",
        "\n",
        "\n",
        "Para preparar los datos de tal forma que estos puedan ser utilizados con algoritmos de *Machine Learning*, es necesario primero hacer un análisis de estos. Esto puede llegar a ser muy lento y requerir tanto conocimientos de Análisis de Datos como de algoritmos de Machine Learning. Para esto suele tratarse la **preparación de los datos de entrada** como un hiperparámetro, escogiendo los algoritmos de ML y los algoritmos de configuración apropiados. Sin embargo, a pesar que esto requiere menor expertiz, esto puede requerir una mucho más rendimiento computacional.\n",
        "\n",
        "El **feature engineering** o **feature extraction** no permite encontrar un punto medio entre ambos casos, usando técnicas de preparación de datos y luego luego ajustar y evaluar un modelo en estos datos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiZ0CPtMWGmn"
      },
      "source": [
        "#### &emsp; 2. Conjunto de Datos y Línea Base de Rendimiento\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ixUONXUWSxo"
      },
      "source": [
        "#### &emsp;&emsp; 2.1. Dataset de Clasificación de Vino\n",
        "\n",
        "* Contamos con un conjunto de datos para clasificación de vinos.\n",
        "* Este cuenta con 13 variables de entrada.\n",
        "* La variable de salida divide los vinos en 3 categorías."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDxU7qbnd7pT",
        "outputId": "0469801e-cfdd-4c0c-8f53-09ebaeee6f91"
      },
      "source": [
        "# example of loading and summarizing the wine dataset\n",
        "from pandas import read_csv\n",
        "\n",
        "# ubicación del dataset\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/wine.csv'\n",
        "\n",
        "# carga el dataset como un data frame\n",
        "df = read_csv(url, header=None)\n",
        "\n",
        "# recuperar el arreglo numpy\n",
        "data = df.values\n",
        "\n",
        "# dividir las columnas en variables de entrada y de salida\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "\n",
        "# resumen de los datos cargados\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(178, 13) (178,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA07VPcEWpR3"
      },
      "source": [
        "#### &emsp;&emsp; 2.2 Rendimiento del Modelo Base\n",
        "\n",
        "Como línea base en la clasificación de vinos usaremos el **modelo de Regresión Logística**\n",
        "\n",
        "Como la biblioteca scikit-learn requiere requiere que las entradas sean numéricas y que la variable objetivo esté etiquetada, entonces esto será realizado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BMkj4WsgzTa"
      },
      "source": [
        "&emsp;&emsp;Cargamos las dependencias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRieCmslgyvl"
      },
      "source": [
        "# baseline model performance on the wine dataset\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from pandas import read_csv\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dymT_gVug7Kg"
      },
      "source": [
        "&emsp;&emsp;Preparación mínima de los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ARCNPX_f8UE"
      },
      "source": [
        "# conjunto de datos mínimamente preparado\n",
        "X = X.astype('float')\n",
        "y = LabelEncoder().fit_transform(y.astype('str'))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvS4xwljhEz9"
      },
      "source": [
        "&emsp;&emsp;Definimos nuestro modelo de predicción"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUuOAz56hJP5"
      },
      "source": [
        "# define the model\n",
        "model = LogisticRegression(solver='liblinear')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzVtuLGZhcmH"
      },
      "source": [
        "&emsp;&emsp;Usando el modelo de validación cruzada estratificada k-fold repetida con 10 folds y 3 repeticiones y lo evaluaremos utilizando el ***accuracy*** de la clasificaión."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jG6y7udpiOaE",
        "outputId": "323b3ee3-8fb4-4138-de2a-44a3636f6186"
      },
      "source": [
        "model = LogisticRegression(solver='liblinear')\n",
        "\n",
        "# define el procedimiento de validación cruzada\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\n",
        "# se evalua el modelo\n",
        "scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "\n",
        "# reporte de rendimiento\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.955 (0.049)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxM-5eOfXJVb"
      },
      "source": [
        "#### &emsp; 3. Enfoque de Extracción de Variables para Preparación de Datos\n",
        "\n",
        "Para mejorar el rendimiento de nuestro modelo, es necesario usar el enfoque de extracción de variables para preparación de datos para lo que tenemos que seleccionar las técnicas de preparación más apropiadas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ws-AQqTHlA60"
      },
      "source": [
        "Dependencias:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eg4Snt1WlDC1"
      },
      "source": [
        "# data preparation as feature engineering for wine dataset\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import TruncatedSVD"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTeQcIhEk3BK"
      },
      "source": [
        "# transforms for the feature union\n",
        "transforms = list()\n",
        "transforms.append(('mms', MinMaxScaler()))\n",
        "transforms.append(('ss', StandardScaler()))\n",
        "transforms.append(('rs', RobustScaler()))\n",
        "transforms.append(('qt', QuantileTransformer(n_quantiles=100, output_distribution='normal')))\n",
        "transforms.append(('kbd', KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')))\n",
        "transforms.append(('pca', PCA(n_components=7)))\n",
        "transforms.append(('svd', TruncatedSVD(n_components=7)))\n",
        "\n",
        "# create the feature union\n",
        "fu = FeatureUnion(transforms)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cKY4kazlLOl"
      },
      "source": [
        "Ahora uniremos las variables para entrenar nuestro modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5l9Y_O3lQXj"
      },
      "source": [
        "# se define el modelo\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "\n",
        "# se define el pipeline\n",
        "steps = list()\n",
        "steps.append(('fu', fu))\n",
        "steps.append(('m', model))\n",
        "pipeline = Pipeline(steps=steps)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wy93cajLluKr",
        "outputId": "9d762504-6d40-4391-96b8-71bb1376ad5d"
      },
      "source": [
        "# define the cross-validation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "# report performance\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.968 (0.037)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_AdmSsil3SS"
      },
      "source": [
        "Como se obsserva, el rendimiento de nuestro modelo, debido a la extracción de variables, logró subir su rendimiento a un 0.968."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbpsvM3xmh0O"
      },
      "source": [
        "Al reducir remover variables de entrada irrelevantes y redundantes podríamos mejorar el rendimiento del modelo, como en este caso en que usaremos RFE:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPpuItocnkLE"
      },
      "source": [
        "Requerimientos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6Jk9gWXnnhf"
      },
      "source": [
        "# data preparation as feature engineering with feature selection for wine dataset\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3OLNgXhnp65"
      },
      "source": [
        "Definimos la delección de variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiDNgR7VnVF-",
        "outputId": "92303ab8-f523-43ca-ff21-4dc8b31ff336"
      },
      "source": [
        "# load the dataset\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/wine.csv'\n",
        "df = read_csv(url, header=None)\n",
        "data = df.values\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "\n",
        "# minimally prepare dataset\n",
        "X = X.astype('float')\n",
        "y = LabelEncoder().fit_transform(y.astype('str'))\n",
        "\n",
        "# transforms for the feature union\n",
        "transforms = list()\n",
        "transforms.append(('mms', MinMaxScaler()))\n",
        "transforms.append(('ss', StandardScaler()))\n",
        "transforms.append(('rs', RobustScaler()))\n",
        "transforms.append(('qt', QuantileTransformer(n_quantiles=100, output_distribution='normal')))\n",
        "transforms.append(('kbd', KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')))\n",
        "transforms.append(('pca', PCA(n_components=7)))\n",
        "transforms.append(('svd', TruncatedSVD(n_components=7)))\n",
        "\n",
        "# create the feature union\n",
        "fu = FeatureUnion(transforms)\n",
        "\n",
        "# define the feature selection\n",
        "rfe = RFE(estimator=LogisticRegression(solver='liblinear'), n_features_to_select=15)\n",
        "\n",
        "# define the model\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "\n",
        "# define the pipeline\n",
        "steps = list()\n",
        "steps.append(('fu', fu))\n",
        "steps.append(('rfe', rfe))\n",
        "steps.append(('m', model))\n",
        "pipeline = Pipeline(steps=steps)\n",
        "\n",
        "# define the cross-validation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\n",
        "# evaluate model\n",
        "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "\n",
        "# report performance\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.989 (0.022)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xh6sDGYyob9P"
      },
      "source": [
        "Se puede ver una mejora en de desempeño del modelo al haber filtrado las variables poco relevantes y las reduntantes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3sKOhA9EQdC"
      },
      "source": [
        "### &emsp; 2. Posible uso que le dará a esta materia como profesional.\n",
        "\n",
        "* En mi futuro como profesional en Ciencias de la Computación, podré mejorar mis algoritmos aplicando las técnicas más adecuadas de forma estructurada con el objetivo de llegar al modelo más simple, pero a la vez preciso y eficiente posible y esto será gracias a la reducción de dimensionalidad de mis conjuntos de datos y al filtrar aquellas variables que me podrían generar problemas en el entrenamiento al introducir ruido."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83HPBJHlNbk9"
      },
      "source": [
        "### &emsp; 3. Aspectos que no le quedan claros, o que quisiera conocer mejor.\n",
        "\n",
        "* No me quedó muy claro exactamente qué es lo que hacen las funciones `MinMaxScaler()`, `StandardScaler()` y `RobustScaler()`, por lo que me daré a la tarea de investigar más al respecto en documentación complementaria.\n"
      ]
    }
  ]
}