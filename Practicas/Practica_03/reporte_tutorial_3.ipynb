{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0nuzkbPqVtY"
      },
      "source": [
        "## Práctica 03 &emsp;(23 de Septiembre)\n",
        "## &emsp;Parte 03\n",
        "## &emsp;Daniel Ricardo Ramírez Umaña, B45675"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkSqsttQqZwa"
      },
      "source": [
        "## Parte 03 <br>Selección de Variables para Aprendizaje Automático (ML)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUVHWwRIql7n"
      },
      "source": [
        "### &emsp;1. Lo aprendido:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytkFNH8Uq6-q"
      },
      "source": [
        "#### &emsp;&emsp; 1. Selección de Variables para Aprendizaje Mecánico\n",
        "\n",
        "Este es un proceso en el que la selección de las variables o características de los datos son seleccionados de manera automatizada y en las que se escogen aquellas que mejor contribuyen a la variable de predicción.\n",
        "\n",
        "La selección de variables antes de modelar los datos nos ofrece 3 beneficios:\n",
        "* **Reduce el Sobreajuste (overfitting):**<br> Menos datos redundantes = Menos oportunidades de tomar decisiones basadas en ruido.\n",
        "* **Mejora la Precisión (accuracy):**<br> Menos datos erroneos = Mejora en precisión de los modelos.\n",
        "* **Reduce el tiempo de entrenamiento:**<br> Menos datos = Algoritmos entrenan más rápido."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCc-e3ybrNoX"
      },
      "source": [
        "#### &emsp;&emsp;&emsp; 1.1. Selección Univariante\n",
        "\n",
        "Con la función de `SelectBest()` de **scikit-learn** podemos escoger un número específico de variables (aquellas que tengan una mayor relación con la variable de salida. Además muchos test estadísticos pueden ser utilizados de forma adicional con este método de selección.\n",
        "\n",
        "Para este caso se seleccionarán las mejores 4 variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DteKgy4TwkTd",
        "outputId": "baffb8a6-d665-42a8-b5c6-60b8a2e921c2"
      },
      "source": [
        "# Feature Selection with Univariate Statistical Tests\n",
        "from pandas import read_csv\n",
        "from numpy import set_printoptions\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "\n",
        "# se cargan los datos\n",
        "filename = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
        "\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "\n",
        "# extracción de variables\n",
        "test = SelectKBest(score_func=f_classif, k=4)\n",
        "fit = test.fit(X, Y)\n",
        "\n",
        "# resumen de puntajes\n",
        "set_printoptions(precision=3)\n",
        "print(fit.scores_, '\\n')\n",
        "features = fit.transform(X)\n",
        "\n",
        "# resumen de variables seleccionadas\n",
        "for i in range(0,8):\n",
        "  print(names[i], 'score:', fit.scores_[i])\n",
        "\n",
        "print('\\n',features[0:5,:])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 39.67  213.162   3.257   4.304  13.281  71.772  23.871  46.141] \n",
            "\n",
            "preg score: 39.670227393616116\n",
            "plas score: 213.16175217803828\n",
            "pres score: 3.256950397889028\n",
            "skin score: 4.304380905008516\n",
            "test score: 13.281107531096337\n",
            "mass score: 71.7720721022405\n",
            "pedi score: 23.871300204344593\n",
            "age score: 46.140611238735865\n",
            "\n",
            " [[  6.  148.   33.6  50. ]\n",
            " [  1.   85.   26.6  31. ]\n",
            " [  8.  183.   23.3  32. ]\n",
            " [  1.   89.   28.1  21. ]\n",
            " [  0.  137.   43.1  33. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwiDx1A-04dG"
      },
      "source": [
        "Como se puede ver las 4 variables con puntuaciones más altas fueron *preq*, *plas*, *mass* y *age* y son estas mismas las que fueron seleccionadas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN4nO3WUrWw6"
      },
      "source": [
        "#### &emsp;&emsp;&emsp; 1.2. Eliminación Recursiva de Valiables (RFE)<br> &emsp;&emsp;&emsp;&emsp;&emsp;  Recursive Feature Elimination\n",
        "\n",
        "Este de forma recursiva contruye modelos removiendo distintos atributos, de esta manera combinando atributos y evaluándolos según su precisión (accuracy) identifica aquellos que mejor predicen (en conjunto) el atributo objetivo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rdhTVfr-jzR",
        "outputId": "613f1a75-759a-4a53-f62e-3d6fc0272111"
      },
      "source": [
        "# Feature Extraction with RFE\n",
        "from pandas import read_csv\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# load data\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(url, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "# feature extraction\n",
        "model = LogisticRegression(solver='lbfgs')\n",
        "rfe = RFE(model, 3)\n",
        "fit = rfe.fit(X, Y)\n",
        "print(\"Num Features: %d\" % fit.n_features_)\n",
        "print(\"Selected Features: %s\" % fit.support_)\n",
        "print(\"Feature Ranking: %s\" % fit.ranking_)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Features: 3\n",
            "Selected Features: [ True False False False False  True  True False]\n",
            "Feature Ranking: [1 2 4 5 6 1 1 3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiF44yqE-59V"
      },
      "source": [
        "Al usar esta función noté que los atributos seleccionados aparecen como `True` en la lista de variables seleccionadas y con valor de 1 en la lista de ranking de variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0kg6lMJsSHl"
      },
      "source": [
        "#### &emsp;&emsp;&emsp; 1.3. Análisis de Componentes Principales<br> &emsp;&emsp;&emsp;&emsp;&emsp; Principal Component Análysis (PCA)\n",
        "\n",
        "Este transforma los conjuntos de datos en una forma más comprimida mediante el uso de álgebra lineal, siendo una técnica de reducción. Esta permite escoger el número de dimensiones o componentes principales en el resultado transformado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIABspdAAjic",
        "outputId": "c7d2253c-306a-45bd-b559-9f3dbf393435"
      },
      "source": [
        "import numpy\n",
        "from pandas import read_csv\n",
        "from sklearn.decomposition import PCA\n",
        "# load data\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(url, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "# feature extraction\n",
        "pca = PCA(n_components=3)\n",
        "fit = pca.fit(X)\n",
        "# summarize components\n",
        "print(\"Explained Variance: %s\" % fit.explained_variance_ratio_)\n",
        "print(fit.components_)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance: [0.889 0.062 0.026]\n",
            "[[-2.022e-03  9.781e-02  1.609e-02  6.076e-02  9.931e-01  1.401e-02\n",
            "   5.372e-04 -3.565e-03]\n",
            " [-2.265e-02 -9.722e-01 -1.419e-01  5.786e-02  9.463e-02 -4.697e-02\n",
            "  -8.168e-04 -1.402e-01]\n",
            " [-2.246e-02  1.434e-01 -9.225e-01 -3.070e-01  2.098e-02 -1.324e-01\n",
            "  -6.400e-04 -1.255e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVy59xlIsmee"
      },
      "source": [
        "#### &emsp;&emsp;&emsp; 1.4. Importancia de las Variables\n",
        "\n",
        "Se pueden estimar la importancia de las variables por medio del uso de árboles de decisión en bolsa, como **Random Forest** y **Extra Trees**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXxjptqfBVLF",
        "outputId": "61be52f8-7023-44da-a749-78aaf721e381"
      },
      "source": [
        "# Feature Importance with Extra Trees Classifier\n",
        "from pandas import read_csv\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "# se cargan los datos\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(url, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "\n",
        "# extracción de variables\n",
        "model = ExtraTreesClassifier(n_estimators=10)\n",
        "model.fit(X, Y)\n",
        "print(model.feature_importances_)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.107 0.241 0.095 0.074 0.077 0.142 0.12  0.146]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia0KhT63BzVH"
      },
      "source": [
        "Estos puntajes obtenidos nos hablan de la importancia de cada variable para el modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNHZsCz5DU80"
      },
      "source": [
        "### &emsp; 2. Posible uso que le dará a esta materia como profesional.\n",
        "\n",
        "* En mi futuro como profesional como Científico de Computación, el conocimiento de estos modelos para selección de Variables o Características me generarán un gran valor en cuanto tiempo, rendimiento y precisión de mis modelos al poder eliminar de mis conjuntos de datos aquellas variables que no aportan mucho o que puedrían generarme ruido."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLbAvDQyqnX6"
      },
      "source": [
        "### &emsp; 3. Aspectos que no le quedan claros, o que quisiera conocer mejor.\n",
        "\n",
        "* Algo que no me quedó claro es que a diferencia de **RFE** que evalua la presición de las variables en su comportamiento en combinación, **Univariative Selection** lo hace evaluando cada variable de forma individual, por lo que si existe correlación entre variables para generar mejores resultados, esto no sería tomado en cuenta, por lo que no parece ser el mejor método a escoger. En qué casos sí se podría considerar dado que RFE nos puede garantizar un mejor filtrado al también considerar las combinaciones de variables."
      ]
    }
  ]
}